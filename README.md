# AI Generated Code Examination - Code Analysis

## Description
This repository contains the evaluation of the experiment "Examination of Code generated by Large Language Models", conducted in September 2023. A paper based on this study will be published soon.   
Results from conducted tests and scripts used to evaluate this data are included here, alongside all study outcomes as images. The official source code for the study is available in a separate [repository](https://github.com/tguttzeit/AI-Code-Examination).   
As the experiment is based on the bachelor thesis "Code Correctness and Quality in the Era of AI Code Generation" by Emilia Hannson & Oliwer Ellréus from 2023, the evaluation is partly similar to its [repository](https://github.com/emiliaajax/ai-tools-analysis). 

## Authors
- Robin Beer ([@BoLer1](https://github.com/BoLer1))
- Alexander Feix ([@alexanderfeix](https://github.com/alexanderfeix))
- Tim Guttzeit ([@tguttzeit](https://github.com/tguttzeit))
- Vincent Müller ([@vincent-mueller](https://github.com/vincent-mueller))
- Tamara Muras ([@t-muras](https://github.com/t-muras))
- Maurice Rauscher ([@mrm021](https://github.com/mrm021))
- Florian Schäffler ([@schaefflerf](https://github.com/schaefflerf))
